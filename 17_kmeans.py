# -*- coding: utf-8 -*-
"""17. KMeans.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aWXv79NmnRAchBusn4v4_yCmoK7vl12N

# **1. Clusters(클러스터)**
* 유사한 특성을 가진 개체들의 집합
* 고객분류, 유전자 분석, 이미지 분할
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
# make_blobs: 데이터셋 만들 때 사용

X, y = make_blobs(n_samples=100, centers=3, random_state=2023) # 100개의 샘플, 3개의 클래스 (지도학습)

X = pd.DataFrame(X)
X

y # 0, 1, 2

sns.scatterplot(x=X[0], y=X[1], hue=y)

from sklearn.cluster import KMeans

km = KMeans(n_clusters=3) # 3개의 군집으로 나눠라
km.fit(X) # 비지도 학습
pred = km.predict(X)

sns.scatterplot(x=X[0], y=X[1], hue=pred)

km = KMeans(n_clusters=5)
km.fit(X)
pred = km.predict(X)

sns.scatterplot(x=X[0], y=X[1], hue=pred) # 최적의 K를 찾는 것이 중요함

# K값 찾기
# 평가값: 하나의 클러스터 안에 중심점으로부터 각각의 데이터 거리를 합한 값의 평균
km.inertia_

inertia_list = []

for i in range(2, 11):
  km = KMeans(n_clusters=i)
  km.fit(X)
  inertia_list.append(km.inertia_)

inertia_list # 값이 적을수록 좋음

sns.lineplot(x=range(2, 11), y=inertia_list)
# 군집 개수를 늘리면 거리가 줄어들 수 밖에 없음
# 거리가 급격하게 낮아지는 지점이 최적 (엘보우 메서드)

"""# **2. Marketing 데이터셋**"""

mkt_df = pd.read_csv('/content/drive/MyDrive/컴퓨터비전_시즌2/3. 데이터 분석/Data/marketing.csv')
mkt_df

# 고객분류 by 비지도
mkt_df.info()

"""* ID: 고객 아이디
* Year_Birth: 출생 연도
* Education: 학력
* Marital_Status: 결혼 여부
* Income: 소득
* Kidhome: 어린이 수
* Teenhome: 청소년 수
* Dt_Customer: 고객 등록일
* Recency: 마지막 구매일로부터 경과일
* MntWines: 와인 구매액
* MntFruits: 과일 구매액
* MntMeatProducts: 육류 구매액
* MntFishProducts: 어류 구매액
* MntSweetProducts: 단맛 제품 구매액
* MntGoldProds: 골드 제품 구매액
* NumDealsPurchases: 할인 행사 구매 수
* NumWebPurchases: 웹에서 구매 수
* NumCatalogPurchases: 카탈로그에서 구매 수
* NumStorePurchases: 매장에서의 구매 수
* NumWebVisitsMonth: 월별 웹 방문 수
* Complain: 불만 여부
"""

mkt_df.drop('ID', axis=1, inplace=True)

mkt_df.describe()

mkt_df.sort_values('Year_Birth')

mkt_df.sort_values('Income', ascending=False)

# mkt_df = mkt_df[mkt_df['Income'] < 200000] -> 이렇게 이상치를 처리하면 NaN도 함께 삭제됨
mkt_df = mkt_df[mkt_df['Income'] != 666666] # NaN 살리기

mkt_df.sort_values('Income', ascending=False)

mkt_df.isna().mean() # 1% 밖에 안되니까 그냥 지우기로

mkt_df = mkt_df.dropna()

mkt_df

mkt_df.info()

# mkt_df['Dt_Customer'] = pd.to_datetime(mkt_df['Dt_Customer']) 년월일시 순서가 아니기 때문에 에러남
mkt_df['Dt_Customer'] = pd.to_datetime(mkt_df['Dt_Customer'], format='%d-%m-%Y')

mkt_df.info()

# 마지막으로 가입된 사람을 기준으로 데이터의 가입 날짜(달)의 차를 구하기
# pass_month
mkt_df['pass_month'] = (mkt_df['Dt_Customer'].max().year * 12 + mkt_df['Dt_Customer'].max().month) - (mkt_df['Dt_Customer'].dt.year * 12 + mkt_df['Dt_Customer'].dt.month)
mkt_df

mkt_df.drop('Dt_Customer', axis=1, inplace = True)

mkt_df.head()

# 와인 + 과일 + 육류 + 어류 + 단맛 + 골드
# 각각도 중요하기 때문에 각각도 남김
# Total_mnt
mkt_df['Total_mnt'] = mkt_df[['MntWines',	'MntFruits', 'MntMeatProducts', 'MntFishProducts',	'MntSweetProducts',	'MntGoldProds']].sum(axis=1)
mkt_df.head()

# KidHome + TeenHome
# 합친 후 각각 제거
mkt_df['Children'] = mkt_df[['Kidhome', 'Teenhome']].sum(axis=1)
mkt_df.head()

mkt_df.drop(['Kidhome', 'Teenhome'], axis=1, inplace=True)
mkt_df.head()

mkt_df.info()

mkt_df['Education'].value_counts() # 바로 원핫인코딩

mkt_df['Marital_Status'].value_counts() # 파트너 있없으로 나누기

mkt_df['Marital_Status'] = mkt_df['Marital_Status'].replace({
    'Married':'Partner',
    'Together':'Partner',
    'Single':'Single',
    'Divorced':'Single',
    'Widow':'Single',
    'Alone':'Single',
    'Absurd':'Single',
    'YOLO':'Single'
})

mkt_df['Marital_Status'].value_counts()

mkt_df.info()

mkt_df = pd.get_dummies(mkt_df, columns=['Education', 'Marital_Status'])
mkt_df.head()

# 범주형 변수(0|1, true|fales), 이미 스케일링한 것들 -> 스케일링 X
# 그 외에는 가중치를 통일하고 싶다면 스케일링 진행
from sklearn.preprocessing import StandardScaler

ss = StandardScaler()

mkt_df.info()

ss.fit_transform(
    mkt_df[['Year_Birth', 'Income', 'Recency', 'MntWines', 'MntFruits',
            'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts',
            'MntGoldProds', 'NumDealsPurchases', 'NumWebPurchases',
            'NumCatalogPurchases', 'NumStorePurchases', 'NumWebVisitsMonth',
            'Complain', 'pass_month', 'Total_mnt', 'Children']]
)

# 스케일링 결과 넣어주기
mkt_df[['Year_Birth', 'Income', 'Recency', 'MntWines', 'MntFruits',
            'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts',
            'MntGoldProds', 'NumDealsPurchases', 'NumWebPurchases',
            'NumCatalogPurchases', 'NumStorePurchases', 'NumWebVisitsMonth',
            'Complain', 'pass_month', 'Total_mnt', 'Children']] = ss.fit_transform(
    mkt_df[['Year_Birth', 'Income', 'Recency', 'MntWines', 'MntFruits',
            'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts',
            'MntGoldProds', 'NumDealsPurchases', 'NumWebPurchases',
            'NumCatalogPurchases', 'NumStorePurchases', 'NumWebVisitsMonth',
            'Complain', 'pass_month', 'Total_mnt', 'Children']]
)

mkt_df.head()

"""# **KMeans**
* K개의 중심점을 찍은 후에 이 중심점에서 각 점간의 거리의 합이 가장 최소가 되는 중심점 K의 위치를 찾고, 이 중심점에서 가까운 점들을 중심점을 기준으로 묶는 알고리즘 (k개의 클러스터의 수는 정해줘야 함)

<center><img src='https://media.geeksforgeeks.org/wp-content/uploads/20190812011831/Screenshot-2019-08-12-at-1.09.42-AM.png'></center>

* 군집의 개수(K) 설정 -> 초기 중심점 설정 -> [데이터를 군집에 할당(배정) -> 중심점 재설정(갱신)] 반복
"""

inertia_list = []

for i in range(2, 11):
  km = KMeans(n_clusters=i, random_state=2024)
  km.fit(mkt_df)
  inertia_list.append(km.inertia_)

inertia_list

sns.lineplot(x=range(2, 11), y=inertia_list) # 엘보우 안나타남

"""# **4. 실루엣 스코어(Silhouette Score)**
* 클러스터링의 품질을 평가하는 지표로, 각 데이터 포인트가 자신이 속한 클러스터와 얼마나 유사하고 다른 클러스터와는 얼마나 다른지를 측정
* -1에서 1사이의 값을 가지며, 값이 클수록 클러스터링의 품질이 높다고 볼 수 있음
"""

from sklearn.metrics import silhouette_score

score = []

for i in range(2, 11):
  km = KMeans(n_clusters=i, random_state=2024)
  km.fit(mkt_df)
  pred = km.predict(mkt_df)
  score.append(silhouette_score(mkt_df, pred))

score

sns.lineplot(x=range(2, 11), y=score)

km = KMeans(n_clusters=4, random_state=2024) # 4개로 군집
km.fit(mkt_df)

pred = km.predict(mkt_df)
pred

mkt_df['label'] = pred
mkt_df

mkt_df['label'].value_counts()

"""* 공부해보기
  * K-최근접 이웃(KNN): 거리 기반 모델. 지도 학습 기반 알고리즘. 회귀/분류
  * 나이브 베이즈(Naive Bayes): 베이즈 정리를 적용한 조건부 확률 기반 분류 모델
  * XG부스트: 순차적으로 트리를 만들어 이전 트리로부터 더 나은 트리를 만들어내는 알고리즘. 지도 학습. 회귀/분류
  * lightGBM(LightGBM): 최신 부스팅 모델. 지도 학습. 회귀/분
"""