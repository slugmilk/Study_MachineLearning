# -*- coding: utf-8 -*-
"""15. 서포트 벡터 머신.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rF3F426bLnh0RTz_brimFB8AwVfCsBDt

# **1. 손글씨 데이터셋**
"""

from sklearn.datasets import load_digits

digits = load_digits()

digits.keys()

data = digits['data']
data.shape

target = digits['target']
target.shape

target

import matplotlib.pyplot as plt

_, axes = plt.subplots(2, 5, figsize=(14, 8))

for i , ax in enumerate(axes.flatten()):
    ax.imshow(data[i].reshape((8, 8)), cmap='gray')
    ax.set_title(target[i])

"""# **2. 스케일링(Scaling)**
* 데이터를 특정한 스케일로 통일하는 것
* 다차원의 값들을 비교 분석하기 쉽게 만들어주며, 자료의 오버플로우나 언더플로우를 방지하여 최적화 과정에서의 안정성 및 수렴 속도를 향상
* 데이터를 모델링하기 전에 거치는 것이 좋음

### 2-1. 스케일링의 종류
* StandardScaler: 평균과 표준편차를 사용
* MinMaxScaler: 최소, 최대값이 각각 0과 1이 되도록 스케일링
* RobustScaler: 중앙값과 IQR사용(아웃라이어의 영향을 최소화)
"""

import pandas as pd

movie = {'naver': [2, 4, 10, 7, 3], 'netflix': [1, 5, 3, 2, 5]}

movie = pd.DataFrame(movie)
movie

from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler

min_max_scaler = MinMaxScaler()

min_max_scaler = min_max_scaler.fit_transform(movie)
min_max_scaler

# movie = {'naver': [2, 4, 10, 7, 3], 'netflix': [1, 5, 3, 2, 5]}
pd.DataFrame(min_max_scaler, columns=['naver', 'netflix'])

"""### 2-2. 정규화(Normalization)
* 값의 범위(Scale)을 0~1사이의 값으로 바꿔주는 것
* 학습 전에 Scaling을 하는 것
* 머신러닝, 딥러닝에서 Scale이 큰 Feature의 영향이 비대해지는 것을 방지
* scikit-learn에서 MinMaxScaler 사용
"""

data[0]

min_max_scaler = MinMaxScaler()

scaled_data = min_max_scaler.fit_transform(data)
scaled_data[0]

"""### 2-3. 표준화(Standardization)
* 값의 범위(Scale)를 평균 0, 분산 1이 되도록 바꿔주는 것
* 학습 전에 Scaling 하는 것
* 머신러닝, 딥러닝에서 Scale이 큰 Feature의 영향이 비대해지는 것을 방지
* scikit-learn에서 StandardScaler 사용
"""

import numpy as np

sample = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])
standard_scaler = StandardScaler()
scaled_sample = standard_scaler.fit_transform(sample)
print('원본데이터: ', sample)
print('스케일링 데이터: ', scaled_sample)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(scaled_data, target, test_size=0.2, random_state=2024)

X_train.shape, y_train.shape

X_test.shape, y_test.shape

"""# **3. Support Vector Machine(SVM)**

<center><img src="https://i.imgur.com/l1NQNvD.png" width="700px"></center>

* 두 클래스로부터 최대한 멀리 떨어져 있는 결정 경계를 찾는 분류기
* 특정 조건을 만족하는 동시에 클래스를 분류하는 것을 목표로 함
"""

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

model = SVC()

model.fit(X_train, y_train)

y_pred = model.predict(X_test)

accuracy_score(y_test, y_pred)

print(y_test[10], y_pred[10])

plt.imshow(X_test[10].reshape(8, 8), cmap='gray')
plt.show()

_, axes = plt.subplots(2, 5, figsize=(14, 8))

for i , ax in enumerate(axes.flatten()):
    ax.imshow(X_test[i].reshape((8, 8)), cmap='gray')
    ax.set_title(f'Label:{y_test[i]}, Pred:{y_pred[i]}')

